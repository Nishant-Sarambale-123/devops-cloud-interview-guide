---

### **Question 133 ‚Äî A pod crashes randomly with OOMKilled. How do you identify and fix this?**

---

#### **Short Explanation**

This question tests your understanding of **Kubernetes resource management and troubleshooting memory issues** ‚Äî specifically, how to diagnose and resolve **Out Of Memory (OOMKilled)** pod terminations.

---

#### **Answer**

When a pod is **OOMKilled**, it means the container exceeded its **memory limit**.
You can identify it using `kubectl describe pod` and fix it by **analyzing memory usage**, **optimizing the application**, or **adjusting memory limits** in the pod spec.

---

#### **Detailed Explanation**

##### üß© What is OOMKilled?

OOMKilled stands for **Out Of Memory Killed** ‚Äî the **Kubernetes kubelet** or the **Linux kernel OOM killer** terminates the container because it tried to use more memory than allowed by its **limit**.

---

### üß† **Step-by-Step Troubleshooting**

#### **1. Check Pod Events**

```bash
kubectl describe pod <pod-name>
```

Look for:

```
State:          Terminated
Reason:         OOMKilled
Last State:     Terminated
Exit Code:      137
```

Exit code **137** = terminated due to OOM.

---

#### **2. Check Memory Usage**

Use metrics from **Prometheus**, **Grafana**, or the command:

```bash
kubectl top pod <pod-name> --namespace <ns>
```

If memory usage constantly hits the limit, it‚Äôs under-provisioned.

---

#### **3. Verify Pod Resource Limits**

Inspect your YAML:

```yaml
resources:
  requests:
    memory: "256Mi"
  limits:
    memory: "512Mi"
```

If the app uses >512Mi, Kubernetes kills it.
‚úÖ Fix: Increase the `limits.memory` or optimize app memory usage.

---

#### **4. Check for Memory Leaks**

* Look at application-level metrics (e.g., via Prometheus, APM).
* For Java: analyze heap dump (`jmap`, `jvisualvm`).
* For Python/Node: use memory profilers.

---

#### **5. Enable Resource Monitoring**

Use **cAdvisor**, **Prometheus**, or **kubectl metrics-server** to track pod memory trends over time.

---

#### **6. Consider Adding a `Startup Probe`**

If the pod uses high memory temporarily during startup, set a **startupProbe** to prevent premature restarts:

```yaml
startupProbe:
  httpGet:
    path: /healthz
    port: 8080
  failureThreshold: 30
  periodSeconds: 10
```

---

### ‚öôÔ∏è **Fixing the Issue**

| **Root Cause**           | **Fix**                                  |
| ------------------------ | ---------------------------------------- |
| Low memory limits        | Increase `resources.limits.memory`       |
| Memory leak              | Debug app, optimize code                 |
| Cache buildup            | Add eviction policy or reduce cache size |
| High concurrency         | Tune threads or async handling           |
| Insufficient node memory | Scale nodes or reschedule workloads      |

---

### üîç **Example**

```yaml
resources:
  requests:
    memory: "512Mi"
  limits:
    memory: "1Gi"
```

If logs show OOMKilled, raise limit to `"1.5Gi"` and monitor again.

---

### **Summary Table**

| **Check**     | **Command / Tool**     | **Purpose**        |
| ------------- | ---------------------- | ------------------ |
| Pod status    | `kubectl describe pod` | Confirm OOMKilled  |
| Metrics       | `kubectl top pod`      | Check memory usage |
| Resource spec | YAML file              | Verify limits      |
| App memory    | APM / profiler         | Find leaks         |
| Monitoring    | Prometheus, Grafana    | Track memory trend |

---

#### **Key Takeaway**

> OOMKilled = memory limit exceeded. Always **monitor usage**, **tune resource limits**, and **fix memory leaks** to prevent repeated pod crashes.

---

Would you like me to continue with **Question 134 ‚Äî ‚ÄúYou got woken up at 2 AM by false alarms. What‚Äôs your strategy to reduce noise?‚Äù**
