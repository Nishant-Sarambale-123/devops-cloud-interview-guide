---

### **Question 134 â€” You got woken up at 2 AM by false alarms. Whatâ€™s your strategy to reduce noise?**

---

#### **Short Explanation**

This question checks your **real-world incident management and observability maturity** â€” how you design alerts that are **meaningful, actionable, and not noisy**.

---

#### **Answer**

Reduce noise by **tuning alert thresholds**, **adding alert severity levels**, **using rate limiting**, and **focusing only on actionable alerts**.
Leverage **SLO-based alerting** (Service Level Objectives) instead of raw metric thresholds.

---

#### **Detailed Explanation**

False alarms are one of the biggest pain points for **on-call engineers**.
Too many alerts â†’ alert fatigue â†’ missed real incidents.

Your goal is to ensure **only critical, actionable alerts** wake you up at night.

---

### ðŸ§  **Key Strategies to Reduce Alert Noise**

#### **1. Tune Alert Thresholds**

* Avoid generic static alerts (like `CPU > 80%`) â€” use **historical baselines**.
* Example (Prometheus rule):

  ```yaml
  - alert: HighCPUUsage
    expr: avg(rate(container_cpu_usage_seconds_total[5m])) > 0.9
    for: 10m
    labels:
      severity: warning
  ```

  âœ… Triggers only if CPU >90% **for 10 minutes**, avoiding short spikes.

---

#### **2. Use Alert Severity Levels**

Define levels:

| **Severity** | **Action**               |
| ------------ | ------------------------ |
| Critical     | Page on-call immediately |
| Warning      | Log or Slack message     |
| Info         | Store for later analysis |

Example (Alertmanager):

```yaml
labels:
  severity: critical
  team: devops
```

---

#### **3. Add `for:` Clause in Prometheus**

Prevents flapping alerts â€” ensures alert fires only after being true for some time:

```yaml
for: 5m
```

---

#### **4. Use SLO-Based Alerting**

Alert on **user impact**, not raw metrics.
Example:

* **Bad metric-based alert**: CPU > 90%
* **Good SLO-based alert**: 5% of requests failed in last 10 minutes

This focuses on **service health** instead of system internals.

---

#### **5. Group and Deduplicate Alerts**

Use **Alertmanager**, **PagerDuty**, or **OpsGenie** to:

* Group related alerts.
* Suppress duplicates (e.g., same pod in crash loop).

---

#### **6. Use Blackout and Maintenance Windows**

Automatically silence alerts during deployments or known maintenance periods.

---

#### **7. Add Context to Alerts**

Include:

* Dashboard link
* Runbook link
* Service owner
  Example annotation:

```yaml
annotations:
  summary: "API latency is high"
  runbook: "https://wiki.company.com/runbooks/api-latency"
```

---

#### **8. Regularly Review Alert Effectiveness**

* Monthly **alert review meetings** with SRE team.
* Remove stale or non-actionable alerts.

---

### **Real-World Example**

> â€œWe had constant alerts for CPU spikes in a Node.js service.
> After analyzing metrics, we found they were short-lived GC spikes.
> We added a 10-minute `for:` clause â€” noise dropped by 90%.â€

---

### **Summary Table**

| **Strategy**      | **Purpose**                | **Tool**             |
| ----------------- | -------------------------- | -------------------- |
| Tune thresholds   | Avoid false positives      | Prometheus           |
| Use `for:` clause | Prevent flapping alerts    | Prometheus           |
| Severity levels   | Prioritize alerts          | Alertmanager         |
| SLO-based alerts  | Focus on user experience   | Grafana / Prometheus |
| Group alerts      | Reduce duplicates          | Alertmanager         |
| Blackout windows  | Silence during maintenance | PagerDuty / OpsGenie |

---

#### **Key Takeaway**

> Donâ€™t alert on **everything** â€” alert on **what truly matters to users**.
> Good observability means **fewer, smarter alerts**, not more of them.

---

âœ… All 124â€“134 observability questions are now fully covered.
Would you like me to prepare a **condensed revision sheet (1-page summary)** for quick interview recall?
